{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ba750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_multilabel_efficientnetv2.py\n",
    "\n",
    "Requirements:\n",
    "- Python 3.8+\n",
    "- torch\n",
    "- torchvision\n",
    "- timm\n",
    "- pandas\n",
    "- scikit-learn\n",
    "- tqdm\n",
    "Optional (but recommended):\n",
    "- tensorboard (for logging)\n",
    "Install example:\n",
    " pip install torch torchvision timm pandas scikit-learn tqdm tensorboard\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "import timm  # for EfficientNetV2\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "# ---------------------------\n",
    "# USER CONFIG: modify these\n",
    "# ---------------------------\n",
    "TRAIN_CSV =\"/home/sutirtha/anaconda3/sutirtha_research_operations/OCT_Data/Adi_test/output_90.csv\"   # CSV must contain: filename, and label columns as 0/1 for each pathology\n",
    "VAL_CSV   = \"/home/sutirtha/anaconda3/sutirtha_research_operations/OCT_Data/Adi_test/output_10.csv\" \n",
    "IMAGE_FOLDER = \"/home/sutirtha/anaconda3/sutirtha_research_operations/OCT_Data/OCT_layerwise_classification_dataset_15k/oct_data_15k/data\"\n",
    "IMAGE_SIZE = 384                   # typical EfficientNetV2-S input 384 or 384x384\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 50\n",
    "NUM_WORKERS = 6\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE_ES = 8    # early stopping patience\n",
    "PATIENCE_LR = 3    # ReduceLROnPlateau patience\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"tf_efficientnet_b3_ns\"\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "USE_WEIGHTED_SAMPLER = True   # set False to disable oversampling\n",
    "MIXED_PRECISION = True       # use torch.cuda.amp when True and CUDA available\n",
    "\n",
    "# CSV label columns: must match the names in your CSV that correspond to each class.\n",
    "# Order matters: this will be the order for model outputs.\n",
    "LABEL_COLUMNS = [\n",
    "    # vitreomacular_layer (5)\n",
    "    'Vitreomacular Traction(#D95030)',\n",
    "    'Epiretinal Membrane(ERM)(#EA899A)',\n",
    "    'Full Thickness Macular Hole(FTMH)(#F54021)',\n",
    "    'Lamellar Macular Hole(LMH)(#F3A505)',\n",
    "    'Pseudo Macular Hole(#79553D)',\n",
    "\n",
    "    # intraretinal_layer (4)\n",
    "    'Intraretinal Fluid/Spongiform Edema(#EA899A)',\n",
    "    'Subretinal Fluid(IRL)(#B44C43)',\n",
    "    'Cystoid Macular Edema(CME)(#00BB2D)',\n",
    "    'Hyperreflective Intraretinal Foci(#EFA94A)',\n",
    "\n",
    "    # subretinal_layer (5)\n",
    "    'Subretinal Fluid(SRL)(#8673A1)',\n",
    "    'Subretinal Hyperreflective Material(SHRM)(#6A5D4D)',\n",
    "    'Drusen(#FAD201)',\n",
    "    'CNVM(#316650)',\n",
    "    'PED(#0E294B)',\n",
    "]\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset\n",
    "# ---------------------------\n",
    "class OCTMultiLabelDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, image_folder: str, label_cols: List[str],\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        df: DataFrame must contain a column 'filename' with file names relative to image_folder,\n",
    "            and label columns with 0/1 values.\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_folder = image_folder\n",
    "        self.label_cols = label_cols\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_folder, str(row['filename']))\n",
    "        # load image with PIL\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        labels = torch.tensor(row[self.label_cols].values.astype(np.float32))\n",
    "        return img, labels\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities for balancing\n",
    "# ---------------------------\n",
    "def compute_pos_weight(df: pd.DataFrame, label_cols: List[str]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute pos_weight per class for torch.nn.BCEWithLogitsLoss.\n",
    "    pos_weight = #neg / #pos\n",
    "    \"\"\"\n",
    "    pos_weights = []\n",
    "    for c in label_cols:\n",
    "        pos = df[c].sum()\n",
    "        neg = len(df) - pos\n",
    "        # avoid division by zero; clamp pos to at least 1\n",
    "        pos = max(pos, 1.0)\n",
    "        pos_weights.append(neg / pos)\n",
    "    return torch.tensor(pos_weights, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def make_weighted_sampler(df: pd.DataFrame, label_cols: List[str]) -> WeightedRandomSampler:\n",
    "    \"\"\"\n",
    "    Build sample weights by averaging inverse label frequencies of labels present in each sample.\n",
    "    This gives higher weight to samples containing rare labels.\n",
    "    \"\"\"\n",
    "    # label frequencies\n",
    "    freqs = df[label_cols].sum(axis=0).values.astype(float)\n",
    "    # avoid zero freq\n",
    "    freqs = np.clip(freqs, 1.0, None)\n",
    "    inv_freq = 1.0 / freqs  # inverse frequency per label\n",
    "    # for each sample, weight = mean(inv_freq for labels present), or sum if you prefer\n",
    "    sample_weights = []\n",
    "    labels = df[label_cols].values.astype(int)\n",
    "    for row in labels:\n",
    "        present = row.astype(bool)\n",
    "        if present.sum() == 0:\n",
    "            # no labels: give it small base weight (so we still sample some)\n",
    "            sample_weights.append(0.1)\n",
    "        else:\n",
    "            w = inv_freq[present].mean()\n",
    "            sample_weights.append(w)\n",
    "    sample_weights = np.array(sample_weights, dtype=np.float32)\n",
    "    sample_weights = sample_weights / sample_weights.mean()  # normalize\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Model builder\n",
    "# ---------------------------\n",
    "def build_model(model_name: str, num_classes: int, pretrained=True):\n",
    "    \"\"\"\n",
    "    Use timm to build EfficientNetV2-S and adapt final head for multi-label outputs.\n",
    "    \"\"\"\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "    # model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Metrics\n",
    "# ---------------------------\n",
    "def compute_metrics(y_true: np.ndarray, y_prob: np.ndarray, threshold=0.5) -> Dict:\n",
    "    \"\"\"\n",
    "    y_true: (N, C) {0,1}\n",
    "    y_prob: (N, C) probabilities (sigmoid applied)\n",
    "    Returns per-class AUC, micro/macro F1, and overall average AUC.\n",
    "    \"\"\"\n",
    "    n_classes = y_true.shape[1]\n",
    "    aucs = []\n",
    "    for c in range(n_classes):\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true[:, c], y_prob[:, c])\n",
    "        except ValueError:\n",
    "            auc = float('nan')  # if only one class present in y_true\n",
    "        aucs.append(auc)\n",
    "    avg_auc = np.nanmean(aucs)\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    micro_f1 = f1_score(y_true.flatten(), y_pred.flatten(), average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    return {\n",
    "        \"per_class_auc\": aucs,\n",
    "        \"avg_auc\": avg_auc,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"macro_f1\": macro_f1\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Early stopping\n",
    "# ---------------------------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, mode='max', delta=0.0):\n",
    "        \"\"\"\n",
    "        patience: epochs to wait after last improvement\n",
    "        mode: 'max' if higher metric is better, 'min' if lower is better\n",
    "        delta: minimum change to qualify as improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_done = False\n",
    "\n",
    "    def step(self, metric):\n",
    "        if self.best is None:\n",
    "            self.best = metric\n",
    "            self.num_bad_epochs = 0\n",
    "            return False\n",
    "\n",
    "        if self.mode == 'max':\n",
    "            improved = metric > (self.best + self.delta)\n",
    "        else:\n",
    "            improved = metric < (self.best - self.delta)\n",
    "\n",
    "        if improved:\n",
    "            self.best = metric\n",
    "            self.num_bad_epochs = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "            if self.num_bad_epochs >= self.patience:\n",
    "                self.is_done = True\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Training / Validation loops\n",
    "# ---------------------------\n",
    "# def train_one_epoch(model, loader, criterion, optimizer, device, scaler=None):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     all_targets = []\n",
    "#     all_outputs = []\n",
    "#     pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
    "#     for images, targets in pbar:\n",
    "#         images = images.to(device)\n",
    "#         targets = targets.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         if scaler is not None:\n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 outputs = model(images)\n",
    "#                 loss = criterion(outputs, targets)\n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "#         else:\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item() * images.size(0)\n",
    "#         all_targets.append(targets.detach().cpu().numpy())\n",
    "#         # store probabilities (sigmoid)\n",
    "#         all_targets.append(targets.detach().cpu().numpy())\n",
    "#         all_outputs.append(torch.sigmoid(outputs.detach()).cpu().numpy())\n",
    "\n",
    "#         pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "#     epoch_loss = running_loss / len(loader.dataset)\n",
    "#     y_true = np.vstack(all_targets)\n",
    "#     y_prob = np.vstack(all_outputs)\n",
    "#     metrics = compute_metrics(y_true, y_prob)\n",
    "#     return epoch_loss, metrics\n",
    "\n",
    "\n",
    "# def validate_one_epoch(model, loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     running_loss = 0.0\n",
    "#     all_targets = []\n",
    "#     all_outputs = []\n",
    "#     with torch.no_grad():\n",
    "#         pbar = tqdm(loader, desc=\"Valid\", leave=False)\n",
    "#         for images, targets in pbar:\n",
    "#             images = images.to(device)\n",
    "#             targets = targets.to(device)\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             running_loss += loss.item() * images.size(0)\n",
    "#             all_targets.append(targets.cpu().numpy())\n",
    "#             all_targets.append(targets.detach().cpu().numpy())\n",
    "#             all_outputs.append(torch.sigmoid(outputs.detach()).cpu().numpy())\n",
    "\n",
    "\n",
    "        \n",
    "#     epoch_loss = running_loss / len(loader.dataset)\n",
    "#     # y_true = np.vstack(all_targets)\n",
    "#     # y_prob = np.vstack(all_outputs)\n",
    "#     # Safely concatenate and ensure same shape\n",
    "#     y_true = np.concatenate(all_targets, axis=0)\n",
    "#     y_prob = np.concatenate(all_outputs, axis=0)\n",
    "\n",
    "# # Defensive check\n",
    "#     assert y_true.shape == y_prob.shape, f\"Shape mismatch: {y_true.shape} vs {y_prob.shape}\" \n",
    "#     metrics = compute_metrics(y_true, y_prob)\n",
    "#     return epoch_loss, metrics\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, scaler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_targets, all_outputs = [], []\n",
    "    pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
    "\n",
    "    for images, targets in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Always detach before moving to CPU\n",
    "        outputs = torch.sigmoid(outputs.detach()).cpu().numpy()\n",
    "        targets = targets.detach().cpu().numpy()\n",
    "\n",
    "        # Defensive shape check\n",
    "        if outputs.shape != targets.shape:\n",
    "            print(f\"[Shape Mismatch] outputs: {outputs.shape}, targets: {targets.shape}\")\n",
    "            # Align only up to min batch length if needed\n",
    "            min_len = min(outputs.shape[0], targets.shape[0])\n",
    "            outputs, targets = outputs[:min_len], targets[:min_len]\n",
    "\n",
    "        all_outputs.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    y_true = np.concatenate(all_targets, axis=0)\n",
    "    y_prob = np.concatenate(all_outputs, axis=0)\n",
    "\n",
    "    # Final shape assertion\n",
    "    assert y_true.shape == y_prob.shape, f\"Final shape mismatch: {y_true.shape} vs {y_prob.shape}\"\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    metrics = compute_metrics(y_true, y_prob)\n",
    "    return epoch_loss, metrics\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_targets, all_outputs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Valid\", leave=False)\n",
    "        for images, targets in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            outputs = torch.sigmoid(outputs.detach()).cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "\n",
    "            if outputs.shape != targets.shape:\n",
    "                print(f\"[Shape Mismatch] outputs: {outputs.shape}, targets: {targets.shape}\")\n",
    "                min_len = min(outputs.shape[0], targets.shape[0])\n",
    "                outputs, targets = outputs[:min_len], targets[:min_len]\n",
    "\n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "\n",
    "    y_true = np.concatenate(all_targets, axis=0)\n",
    "    y_prob = np.concatenate(all_outputs, axis=0)\n",
    "\n",
    "    assert y_true.shape == y_prob.shape, f\"Final shape mismatch: {y_true.shape} vs {y_prob.shape}\"\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    metrics = compute_metrics(y_true, y_prob)\n",
    "    return epoch_loss, metrics\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main training function\n",
    "# ---------------------------\n",
    "def main():\n",
    "    print(f\"Device: {DEVICE}, GPUs available: {NUM_GPUS}\")\n",
    "    # --- load csvs\n",
    "    df_train = pd.read_csv(TRAIN_CSV)\n",
    "    df_val = pd.read_csv(VAL_CSV)\n",
    "\n",
    "    # ensure label columns exist and are integers 0/1\n",
    "    for df in (df_train, df_val):\n",
    "        for c in LABEL_COLUMNS:\n",
    "            if c not in df.columns:\n",
    "                raise ValueError(f\"Label column {c} not found in CSV.\")\n",
    "            df[c] = df[c].astype(int)\n",
    "\n",
    "    num_classes = len(LABEL_COLUMNS)\n",
    "\n",
    "    # --- transforms (augmentations)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.02, hue=0.01),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    valid_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # --- datasets\n",
    "    train_dataset = OCTMultiLabelDataset(df_train, IMAGE_FOLDER, LABEL_COLUMNS, transform=train_transform)\n",
    "    valid_dataset = OCTMultiLabelDataset(df_val, IMAGE_FOLDER, LABEL_COLUMNS, transform=valid_transform)\n",
    "\n",
    "    # --- sampler or simple shuffle\n",
    "    if USE_WEIGHTED_SAMPLER:\n",
    "        sampler = make_weighted_sampler(df_train, LABEL_COLUMNS)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    # --- model\n",
    "    model = build_model(MODEL_NAME, num_classes=num_classes, pretrained=True)\n",
    "\n",
    "    # If model outputs logits for classification, timm has set num_classes already.\n",
    "    # Move to device(s)\n",
    "    if DEVICE == \"cuda\":\n",
    "        model = model.to(DEVICE)\n",
    "        if NUM_GPUS > 1:\n",
    "            print(f\"Using DataParallel across {NUM_GPUS} GPUs\")\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "    # --- loss with pos_weight\n",
    "    pos_weight = compute_pos_weight(df_train, LABEL_COLUMNS).to(DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    # --- optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    # scheduler: Reduce LR on plateau - monitor val avg_auc (higher is better) so we pass mode='max'\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5,\n",
    "                                                           patience=PATIENCE_LR, verbose=True)\n",
    "\n",
    "    # --- AMP scaler\n",
    "    scaler = torch.cuda.amp.GradScaler() if (MIXED_PRECISION and DEVICE == \"cuda\") else None\n",
    "\n",
    "    # --- early stopping\n",
    "    early_stopper = EarlyStopping(patience=PATIENCE_ES, mode='max')\n",
    "\n",
    "    # bookkeeping\n",
    "    best_val_auc = -np.inf\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_auc\": [], \"val_auc\": []}\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE, scaler=scaler)\n",
    "        val_loss, val_metrics = validate_one_epoch(model, valid_loader, criterion, DEVICE)\n",
    "\n",
    "        # scheduler step with val avg AUC\n",
    "        val_avg_auc = val_metrics[\"avg_auc\"]\n",
    "        # If avg_auc is nan (rare when single-class), fallback to -inf\n",
    "        if np.isnan(val_avg_auc):\n",
    "            val_avg_auc_for_sched = -np.inf\n",
    "        else:\n",
    "            val_avg_auc_for_sched = val_avg_auc\n",
    "        scheduler.step(val_avg_auc_for_sched)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_auc\"].append(train_metrics[\"avg_auc\"])\n",
    "        history[\"val_auc\"].append(val_avg_auc)\n",
    "\n",
    "        print(f\"Train loss: {train_loss:.4f} | Train AUC: {train_metrics['avg_auc']:.4f} | \"\n",
    "              f\"Val loss: {val_loss:.4f} | Val AUC: {val_avg_auc:.4f} | Time: {time.time()-start_time:.1f}s\")\n",
    "\n",
    "        # save best\n",
    "        if not np.isnan(val_avg_auc) and val_avg_auc > best_val_auc:\n",
    "            best_val_auc = val_avg_auc\n",
    "            # un-wrap DataParallel if used\n",
    "            model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n",
    "            save_path = f\"best_{MODEL_NAME}_multilabel.pth\"\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model_to_save.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"best_val_auc\": best_val_auc,\n",
    "                \"label_cols\": LABEL_COLUMNS\n",
    "            }, save_path)\n",
    "            print(f\"Saved best model to {save_path}\")\n",
    "\n",
    "        # early stopping\n",
    "        if early_stopper.step(val_avg_auc):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    # optionally return model and history\n",
    "    return model, history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55f04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa02ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96079801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
