{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "from typing import List,Tuple,Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,dataloader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "import timm \n",
    "from sklearn.metrics import f1_score,accuracy_score,roac_auc_score\n",
    "\n",
    "Train_csv=\"\"\n",
    "Test_csv=\"\"\n",
    "Image_fodler_path=\"\"\n",
    "BATCH_SIZE=16\n",
    "IMAGE_SIZE=224\n",
    "NUM_EPOCHS=50\n",
    "NUM_WORKERS=6\n",
    "LEARNING_RATE=1e-4\n",
    "WEIGHT_DECAY=1e-4\n",
    "PATIENCE_ES=8\n",
    "PATIENCE_LR=3\n",
    "DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME=\"efficientnetv2_s\"\n",
    "NUM_GPUS=torch.cuda.device_count()\n",
    "USE_WEIGHTED_SAMPLER=True\n",
    "MIXED_PRECISION=True\n",
    "\n",
    "\n",
    "LABEL_COLUMNS = [\n",
    "    # vitreomacular_layer (5)\n",
    "    'Vitreomacular Traction(#D95030)',\n",
    "    'Epiretinal Membrane(ERM)(#EA899A)',\n",
    "    'Full Thickness Macular Hole(FTMH)(#F54021)',\n",
    "    'Lamellar Macular Hole(LMH)(#F3A505)',\n",
    "    'Pseudo Macular Hole(#79553D)',\n",
    "\n",
    "    # intraretinal_layer (4)\n",
    "    'Intraretinal Fluid/Spongiform Edema(#EA899A)',\n",
    "    'Subretinal Fluid(IRL)(#B44C43)',\n",
    "    'Cystoid Macular Edema(CME)(#00BB2D)',\n",
    "    'Hyperreflective Intraretinal Foci(#EFA94A)',\n",
    "\n",
    "    # subretinal_layer (5)\n",
    "    'Subretinal Fluid(SRL)(#8673A1)',\n",
    "    'Subretinal Hyperreflective Material(SHRM)(#6A5D4D)',\n",
    "    'Drusen(#FAD201)',\n",
    "    'CNVM(#316650)',\n",
    "    'PED(#0E294B)',\n",
    "]\n",
    "\n",
    "class octdataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, image_folder:str, label_cols: List[str],\n",
    "                 transform=None):\n",
    "        self.df=df.reset_index(drop=True)\n",
    "        self.image_folder=image_folder\n",
    "        self.label_cols=label_cols\n",
    "        self.tranform=transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem_(self,idx):\n",
    "        row=self.df.iloc[idx]\n",
    "        img_path=os.join(self.image_folder,str(row['filename']))\n",
    "        img=Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img=self.transform(img)\n",
    "        labels = torch.tensor(row[self.label_cols].values.astype(np.float32))\n",
    "        return img,labels\n",
    "    \n",
    "     \n",
    "def compute_pos_weight(df: pd.DataFrame, label_cols:List[str]) -> torch.Tensor:\n",
    "    pos_weights=[]\n",
    "    for c in label_cols:\n",
    "        pos=df[c].sum()\n",
    "        neg=len(df)-pos\n",
    "        pos=max(pos,1.0) #use atleast one positive sample to avoid dvision by 0\n",
    "        pos_weights.append(neg/pos)\n",
    "    return torch.tensor(pos_weights,dtype=torch.float32) \n",
    "\n",
    "\n",
    "def make_weighted_sampler(df: pd.DataFrame, label_cols: List[str]) -> WeightedRandomSampler:\n",
    "    \"\"\"\n",
    "    Build sample weights by averaging inverse label frequencies of labels present in each sample.\n",
    "    This gives higher weight to samples containing rare labels.\n",
    "    \"\"\"\n",
    "    # label frequencies\n",
    "    freqs = df[label_cols].sum(axis=0).values.astype(float)\n",
    "    # avoid zero freq\n",
    "    freqs = np.clip(freqs, 1.0, None)\n",
    "    inv_freq = 1.0 / freqs  # inverse frequency per label\n",
    "    # for each sample, weight = mean(inv_freq for labels present), or sum if you prefer\n",
    "    sample_weights = []\n",
    "    labels = df[label_cols].values.astype(int)\n",
    "    for row in labels:\n",
    "        present = row.astype(bool)\n",
    "        if present.sum() == 0:\n",
    "            # no labels: give it small base weight (so we still sample some)\n",
    "            sample_weights.append(0.1)\n",
    "        else:\n",
    "            w = inv_freq[present].mean()\n",
    "            sample_weights.append(w)\n",
    "    sample_weights = np.array(sample_weights, dtype=np.float32)\n",
    "    sample_weights = sample_weights / sample_weights.mean()  # normalize\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_name: str, num_classes: int, pretrained=True):\n",
    "    \"\"\"\n",
    "    Use timm to build EfficientNetV2-S and adapt final head for multi-label outputs.\n",
    "    \"\"\"\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "    # model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_prob: np.ndarray, threshold=0.5) -> Dict:\n",
    "    \"\"\"\n",
    "    y_true: (N, C) {0,1}\n",
    "    y_prob: (N, C) probabilities (sigmoid applied)\n",
    "    Returns per-class AUC, micro/macro F1, and overall average AUC.\n",
    "    \"\"\"\n",
    "    n_classes = y_true.shape[1]\n",
    "    aucs = []\n",
    "    for c in range(n_classes):\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true[:, c], y_prob[:, c])\n",
    "        except ValueError:\n",
    "            auc = float('nan')  # if only one class present in y_true\n",
    "        aucs.append(auc)\n",
    "    avg_auc = np.nanmean(aucs)\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    micro_f1 = f1_score(y_true.flatten(), y_pred.flatten(), average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    return {\n",
    "        \"per_class_auc\": aucs,\n",
    "        \"avg_auc\": avg_auc,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"macro_f1\": macro_f1\n",
    "    }\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, mode='max', delta=0.0):\n",
    "        \"\"\"\n",
    "        patience: epochs to wait after last improvement\n",
    "        mode: 'max' if higher metric is better, 'min' if lower is better\n",
    "        delta: minimum change to qualify as improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_done = False\n",
    "\n",
    "    def step(self, metric):\n",
    "        if self.best is None:\n",
    "            self.best = metric\n",
    "            self.num_bad_epochs = 0\n",
    "            return False\n",
    "\n",
    "        if self.mode == 'max':\n",
    "            improved = metric > (self.best + self.delta)\n",
    "        else:\n",
    "            improved = metric < (self.best - self.delta)\n",
    "\n",
    "        if improved:\n",
    "            self.best = metric\n",
    "            self.num_bad_epochs = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "            if self.num_bad_epochs >= self.patience:\n",
    "                self.is_done = True\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, scaler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_targets, all_outputs = [], []\n",
    "    pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
    "\n",
    "    for images, targets in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Always detach before moving to CPU\n",
    "        outputs = torch.sigmoid(outputs.detach()).cpu().numpy()\n",
    "        targets = targets.detach().cpu().numpy()\n",
    "\n",
    "        #Defensive shape check\n",
    "        if outputs.shape != targets.shape:\n",
    "            print(f\"[Shape Mismatch] outputs: {outputs.shape}, targets: {targets.shape}\")\n",
    "            # Align only up to min batch length if needed\n",
    "            min_len = min(outputs.shape[0], targets.shape[0])\n",
    "            outputs, targets = outputs[:min_len], targets[:min_len]\n",
    "\n",
    "        all_outputs.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    y_true = np.concatenate(all_targets, axis=0)\n",
    "    y_prob = np.concatenate(all_outputs, axis=0)\n",
    "\n",
    "    # Final shape assertion\n",
    "    assert y_true.shape == y_prob.shape, f\"Final shape mismatch: {y_true.shape} vs {y_prob.shape}\"\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    metrics = compute_metrics(y_true, y_prob)\n",
    "    return epoch_loss, metrics\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_targets, all_outputs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Valid\", leave=False)\n",
    "        for images, targets in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            outputs = torch.sigmoid(outputs.detach()).cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "\n",
    "            if outputs.shape != targets.shape:\n",
    "                print(f\"[Shape Mismatch] outputs: {outputs.shape}, targets: {targets.shape}\")\n",
    "                min_len = min(outputs.shape[0], targets.shape[0])\n",
    "                outputs, targets = outputs[:min_len], targets[:min_len]\n",
    "\n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "\n",
    "    y_true = np.concatenate(all_targets, axis=0)\n",
    "    y_prob = np.concatenate(all_outputs, axis=0)\n",
    "\n",
    "    assert y_true.shape == y_prob.shape, f\"Final shape mismatch: {y_true.shape} vs {y_prob.shape}\"\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    metrics = compute_metrics(y_true, y_prob)\n",
    "    return epoch_loss, metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c13ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(f\"Device: {DEVICE}, GPUs available: {NUM_GPUS}\")\n",
    "    # --- load csvs\n",
    "    df_train = pd.read_csv(TRAIN_CSV)\n",
    "    df_val = pd.read_csv(VAL_CSV)\n",
    "\n",
    "    # ensure label columns exist and are integers 0/1\n",
    "    for df in (df_train, df_val):\n",
    "        for c in LABEL_COLUMNS:\n",
    "            if c not in df.columns:\n",
    "                raise ValueError(f\"Label column {c} not found in CSV.\")\n",
    "            df[c] = df[c].astype(int)\n",
    "\n",
    "    num_classes = len(LABEL_COLUMNS)\n",
    "\n",
    "    # --- transforms (augmentations)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.02, hue=0.01),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    valid_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # --- datasets\n",
    "    train_dataset = OCTMultiLabelDataset(df_train, IMAGE_FOLDER, LABEL_COLUMNS, transform=train_transform)\n",
    "    valid_dataset = OCTMultiLabelDataset(df_val, IMAGE_FOLDER, LABEL_COLUMNS, transform=valid_transform)\n",
    "\n",
    "    # --- sampler or simple shuffle\n",
    "    if USE_WEIGHTED_SAMPLER:\n",
    "        sampler = make_weighted_sampler(df_train, LABEL_COLUMNS)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    # --- model\n",
    "    model = build_model(MODEL_NAME, num_classes=num_classes, pretrained=True)\n",
    "\n",
    "    # If model outputs logits for classification, timm has set num_classes already.\n",
    "    # Move to device(s)\n",
    "    if DEVICE == \"cuda\":\n",
    "        model = model.to(DEVICE)\n",
    "        if NUM_GPUS > 1:\n",
    "            print(f\"Using DataParallel across {NUM_GPUS} GPUs\")\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "    # --- loss with pos_weight\n",
    "    pos_weight = compute_pos_weight(df_train, LABEL_COLUMNS).to(DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    # --- optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    # scheduler: Reduce LR on plateau - monitor val avg_auc (higher is better) so we pass mode='max'\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5,\n",
    "                                                           patience=PATIENCE_LR, verbose=True)\n",
    "\n",
    "    # --- AMP scaler\n",
    "    scaler = torch.cuda.amp.GradScaler() if (MIXED_PRECISION and DEVICE == \"cuda\") else None\n",
    "\n",
    "    # --- early stopping\n",
    "    early_stopper = EarlyStopping(patience=PATIENCE_ES, mode='max')\n",
    "\n",
    "    # bookkeeping\n",
    "    best_val_auc = -np.inf\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_auc\": [], \"val_auc\": []}\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE, scaler=scaler)\n",
    "        val_loss, val_metrics = validate_one_epoch(model, valid_loader, criterion, DEVICE)\n",
    "\n",
    "        # scheduler step with val avg AUC\n",
    "        val_avg_auc = val_metrics[\"avg_auc\"]\n",
    "        # If avg_auc is nan (rare when single-class), fallback to -inf\n",
    "        if np.isnan(val_avg_auc):\n",
    "            val_avg_auc_for_sched = -np.inf\n",
    "        else:\n",
    "            val_avg_auc_for_sched = val_avg_auc\n",
    "        scheduler.step(val_avg_auc_for_sched)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_auc\"].append(train_metrics[\"avg_auc\"])\n",
    "        history[\"val_auc\"].append(val_avg_auc)\n",
    "\n",
    "        print(f\"Train loss: {train_loss:.4f} | Train AUC: {train_metrics['avg_auc']:.4f} | \"\n",
    "              f\"Val loss: {val_loss:.4f} | Val AUC: {val_avg_auc:.4f} | Time: {time.time()-start_time:.1f}s\")\n",
    "\n",
    "        # save best\n",
    "        if not np.isnan(val_avg_auc) and val_avg_auc > best_val_auc:\n",
    "            best_val_auc = val_avg_auc\n",
    "            # un-wrap DataParallel if used\n",
    "            model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n",
    "            save_path = f\"best_{MODEL_NAME}_multilabel.pth\"\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model_to_save.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"best_val_auc\": best_val_auc,\n",
    "                \"label_cols\": LABEL_COLUMNS\n",
    "            }, save_path)\n",
    "            print(f\"Saved best model to {save_path}\")\n",
    "\n",
    "        # early stopping\n",
    "        if early_stopper.step(val_avg_auc):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    # optionally return model and history\n",
    "    return model, history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952c230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2178a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class MultiLabelTestDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # columns from 1 onward are labels\n",
    "        self.image_names = self.data.iloc[:, 0].values\n",
    "        self.labels = self.data.iloc[:, 1:].values.astype(float)\n",
    "        self.class_names = list(self.data.columns[1:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = f\"{self.img_dir}/{self.image_names[idx]}\"\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_on_test(model, test_loader, device, class_names, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_targets, all_probs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "            images = images.to(device)\n",
    "            outputs = torch.sigmoid(model(images)).detach().cpu().numpy()\n",
    "            all_probs.append(outputs)\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(all_targets, axis=0)\n",
    "    y_prob = np.concatenate(all_probs, axis=0)\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "    # ----- Compute Metrics -----\n",
    "    metrics = {}\n",
    "    aucs, f1s, precisions, recalls = [], [], [], []\n",
    "    for i, cls in enumerate(class_names):\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true[:, i], y_prob[:, i])\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "        aucs.append(auc)\n",
    "        f1s.append(f1_score(y_true[:, i], y_pred[:, i], zero_division=0))\n",
    "        precisions.append(precision_score(y_true[:, i], y_pred[:, i], zero_division=0))\n",
    "        recalls.append(recall_score(y_true[:, i], y_pred[:, i], zero_division=0))\n",
    "\n",
    "    metrics[\"AUCs\"] = aucs\n",
    "    metrics[\"F1s\"] = f1s\n",
    "    metrics[\"Precisions\"] = precisions\n",
    "    metrics[\"Recalls\"] = recalls\n",
    "    metrics[\"macro_auc\"] = np.nanmean(aucs)\n",
    "    metrics[\"macro_f1\"] = np.nanmean(f1s)\n",
    "\n",
    "    # ----- Print Summary -----\n",
    "    print(\"\\n==== Test Metrics Summary ====\")\n",
    "    print(f\"Macro AUC: {metrics['macro_auc']:.4f}\")\n",
    "    print(f\"Macro F1 : {metrics['macro_f1']:.4f}\")\n",
    "\n",
    "    # ----- Plot 1: Per-Class Metrics -----\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    df_plot = pd.DataFrame({\n",
    "        \"Class\": class_names,\n",
    "        \"AUC\": aucs,\n",
    "        \"F1\": f1s,\n",
    "        \"Precision\": precisions,\n",
    "        \"Recall\": recalls\n",
    "    })\n",
    "    df_plot.set_index(\"Class\").plot(kind=\"bar\", ax=ax)\n",
    "    plt.title(\"Per-Class Performance Metrics\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(rotation=70)\n",
    "    plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ----- Plot 2: ROC Curves -----\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, cls in enumerate(class_names):\n",
    "        try:\n",
    "            fpr, tpr, _ = roc_curve(y_true[:, i], y_prob[:, i])\n",
    "            plt.plot(fpr, tpr, label=f\"{cls} (AUC={aucs[i]:.2f})\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curves by Class\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0411202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee0320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3d586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d47bef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
